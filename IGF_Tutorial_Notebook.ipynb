{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook will serve as a tutorial for implementing and using\n",
    "# IGF, information gain filtration, which is a method proposed by Antonello et. al. \n",
    "# for more effective finetuning of pretrained models. \n",
    "\n",
    "# The tutorial follows three steps:\n",
    "\n",
    "# 1) A dataset of pairs (X, IG(X)) is generated.\n",
    "# 2) A secondary learner is created to infer a function approximation for IG using the dataset created in (1).\n",
    "# 3) The learner created in (2) is used to inform the finetuning process. \n",
    "# We then generate a plot comparing the performance of IGF \n",
    "# compared to standard finetuing without any context filtering.\n",
    "\n",
    "# Each stage of the process can alternatively be skipped with checkpoint files that are also included in the Github repo.\n",
    "\n",
    "# What you will need:\n",
    "# Download the tokenized version of the Wikitext dataset\n",
    "# or another corpus that you would like to finetune on.\n",
    "\n",
    "# Some parts of this script lifted from here:\n",
    "# https://github.com/huggingface/transformers/blob/master/examples/run_glue.py\n",
    "\n",
    "# Prerequisite libraries: \n",
    "\n",
    "import torch\n",
    "import joblib\n",
    "import random\n",
    "import logging\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import (AdamW, GPT2LMHeadModel, GPT2Tokenizer)\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from scipy.stats import ttest_ind\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We use contexts of length 128 for this notebook for efficiency\n",
    "# but this number can be changed as you like\n",
    "\n",
    "CONTEXT_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This first cell handles the first step, generating a dataset of pairs (X, IG(X)).\n",
    "# This is the most time consuming task, and can be skipped by using the provided\n",
    "# pregenerated dataset of pairs. This takes awhile compared to the other steps.\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def compute_real_Q(test_data, model):\n",
    "    '''Computes perplexity of the model on the objective set, test_data'''\n",
    "    model.eval()\n",
    "    eval_batch_size = 1\n",
    "    device = \"cpu\"\n",
    "    #device = next(model.parameters()).device\n",
    "    context = torch.zeros((eval_batch_size, CONTEXT_LEN), dtype=torch.long, device=device)\n",
    "    eval_sampler = SequentialSampler(test_data)\n",
    "    eval_dataloader = DataLoader(test_data, sampler=eval_sampler, batch_size=eval_batch_size)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_examples = 0\n",
    "    for batch in eval_dataloader:\n",
    "        # pad\n",
    "        context.zero_()\n",
    "        for i in range(eval_batch_size):\n",
    "            context[i, :] = batch[i]\n",
    "        eval_loss += model(context, labels=context)[0].sum().item()\n",
    "        nb_eval_examples += batch.size(0)\n",
    "    eval_loss = eval_loss / nb_eval_examples\n",
    "    perplexity = torch.exp(torch.tensor(eval_loss, device=device))\n",
    "    model.train()\n",
    "    return perplexity\n",
    "\n",
    "def train(train_dataset, test_dataset, model, tokenizer,  max_steps, r_penalty, data_start):\n",
    "    \"\"\" Train the model \"\"\"\n",
    "    # Initialize the model\n",
    "    device = torch.device('cpu')\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler)\n",
    "\n",
    "    t_total = max_steps\n",
    "    num_train_epochs = max_steps // (len(train_dataset)) + 1\n",
    "    past_Q = torch.tensor(0.0, device=device)\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    global_step = 0\n",
    "    tr_lm_loss, logging_lm_loss = 0.0, 0.0\n",
    "    tr_q_loss, logging_q_loss = 0.0, 0.0\n",
    "    context = torch.zeros((1, CONTEXT_LEN), dtype=torch.long, device=device)\n",
    "    loss = nn.MSELoss()\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2-medium', output_hidden_states=True)\n",
    "    config = model.config\n",
    "    torch.save(model.state_dict(), \"gpt2_weights2.torch\")\n",
    "\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    lm_optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5, eps=1e-8)\n",
    "    first_time = True\n",
    "    lm_scheduler = get_linear_schedule_with_warmup(lm_optimizer, 0, t_total)\n",
    "    contexts = []\n",
    "    past_perps = []\n",
    "    real_perps = []\n",
    "    past_perps2 = []\n",
    "    features = []\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    for story in train_dataset:\n",
    "        for step, example in enumerate(story[0]):\n",
    "            # Periodically save the stored (X, IG(X)) pairs\n",
    "            if global_step % 1000 == 0 and global_step > 1:\n",
    "                data = (contexts, past_perps, real_perps, past_perps2)\n",
    "                data_final = []\n",
    "                allp = []\n",
    "                for i in range(len(data[1])):\n",
    "                    allp.append(data[1][i] - data[2][i])\n",
    "                    data_final.append((data[0][i], data[1][i] - data[2][i], data[3][i]))\n",
    "\n",
    "                avg = np.array(allp).mean()\n",
    "                std = np.array(allp).std()\n",
    "\n",
    "                for i in range(len(data_final)):\n",
    "                    data_final[i] = (data_final[i][0], (data_final[i][1] - avg) / std, data_final[i][2])\n",
    "                joblib.dump(data_final, \"IGF_values_gpt_med.jbl\")\n",
    "                del data_final\n",
    "                del avg\n",
    "                del std\n",
    "                del allp\n",
    "                del data\n",
    "            if True:\n",
    "                # Reset the model to the original pretrained GPT-2 weights after each iteration\n",
    "                del model\n",
    "                del optimizer_grouped_parameters\n",
    "                del no_decay\n",
    "                del lm_optimizer\n",
    "                del lm_scheduler\n",
    "                torch.cuda.empty_cache()\n",
    "                model = GPT2LMHeadModel(config=config)\n",
    "                model.load_state_dict(torch.load(\"gpt2_weights2.torch\"))\n",
    "                model.to(device)\n",
    "                no_decay = ['bias', 'LayerNorm.weight']\n",
    "                optimizer_grouped_parameters = [\n",
    "                    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                     'weight_decay': 0.0},\n",
    "                    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                     'weight_decay': 0.0}\n",
    "                ]\n",
    "                lm_optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5, eps=1e-8)\n",
    "                lm_scheduler = get_linear_schedule_with_warmup(lm_optimizer, 0, t_total)\n",
    "                context = torch.zeros((1, CONTEXT_LEN), dtype=torch.long, device=device)\n",
    "            torch.cuda.empty_cache()\n",
    "            story2 = random.choice(train_dataset)\n",
    "            # Skip articles if they aren't of sufficient length\n",
    "            if len(story2[0]) < 1026:\n",
    "                #print(\"skipped\")\n",
    "                continue\n",
    "            start = random.randint(0, len(story2[0]) - CONTEXT_LEN - 1)\n",
    "            context[0, :] = story2[0][start:start + CONTEXT_LEN]\n",
    "            lm_optimizer.zero_grad()\n",
    "            outputs = model(context, labels=context)\n",
    "            lm_loss = outputs[0]\n",
    "            # Compute perplexity of initial model\n",
    "            if first_time:\n",
    "                past_perp = compute_real_Q(test_dataset, model)\n",
    "                first_time = False\n",
    "            past_perp2 = compute_real_Q(context, model)\n",
    "            model.train()\n",
    "            lm_loss.backward()\n",
    "            # Do LM backprop\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n",
    "            lm_optimizer.step()\n",
    "            lm_scheduler.step()  # Update learning rate schedule\n",
    "            features = []\n",
    "            # Compute perplexity after backpropogating on the selected context\n",
    "            real_perp = compute_real_Q(test_dataset, model)\n",
    "            if (global_step + 1) % 50 == 0:\n",
    "                print(\"Number of Samples: \", global_step + 1)\n",
    "            past_perps.append(past_perp.item())\n",
    "            past_perps2.append(past_perp2.item())\n",
    "            real_perps.append(real_perp.item())\n",
    "            contexts.append(np.array(context.cpu()))\n",
    "            del outputs\n",
    "            del real_perp\n",
    "            del context\n",
    "            global_step += 1\n",
    "            del lm_loss\n",
    "\n",
    "            if max_steps > 0 and global_step > max_steps:\n",
    "                break\n",
    "        if max_steps > 0 and global_step > max_steps:\n",
    "            break\n",
    "\n",
    "    return global_step, tr_lm_loss / global_step\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #Generate objective set and training set\n",
    "    data_start = 42\n",
    "    # Designate the first 100 articles to be used as our objective set\n",
    "    test_data = joblib.load(\"tokenized_stories_train_wikitext103.jbl\")[:100]\n",
    "    # The rest of the articles in the training set will be used to sample for finetuning\n",
    "    train_data = joblib.load(\"tokenized_stories_train_wikitext103.jbl\")[100:]\n",
    "    set_seed(3)\n",
    "    test_data_sliced = []\n",
    "    # Randomly generate our objective set\n",
    "    for test_example in test_data:\n",
    "        if len(test_example[0]) > 1026:\n",
    "            start = random.randint(0, len(test_example[0]) - CONTEXT_LEN - 1)\n",
    "            test_data_sliced.append(test_example[0, start:start + CONTEXT_LEN])\n",
    "    joblib.dump(test_data_sliced, \"objective_set.jbl\")\n",
    "    set_seed(int(data_start))\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2', output_hidden_states=True)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    train(train_data, test_data_sliced, model, tokenizer, 4000, torch.tensor(0.001), int(data_start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This next cell handles the second step, which takes the generated (X, IG(X)) pairs and \n",
    "# generates a secondary learner that regresses the IG function.\n",
    "# This can be evaluated using the pregeneated IGF_values.jbl file or the one generated in the\n",
    "# cell above\n",
    "import joblib\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def train2(dqn, train_dataset, max_steps, batch_size):\n",
    "    \"\"\" Train the model \"\"\"\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    # We will use the first 512 pairs from our dataset as a test set for \n",
    "    # our secondary learner and the rest to train\n",
    "    test_dataset = train_dataset[:512]\n",
    "    train_dataset = train_dataset[512:]\n",
    "\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\n",
    "    test_sampler = SequentialSampler(test_dataset)\n",
    "    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    global_step = 0\n",
    "    loss = nn.MSELoss()\n",
    "    test_loss = nn.MSELoss(reduction='sum')\n",
    "    dqn.to(device)\n",
    "    q_optimizer = torch.optim.Adam(dqn.parameters(), lr=0.00001)\n",
    "\n",
    "    dqn.train()\n",
    "\n",
    "    epoch = 0\n",
    "\n",
    "    best_test_loss = float('inf')\n",
    "    # Iterate through batches until we've used max_steps batches\n",
    "    while True:\n",
    "        print('Epoch', epoch)\n",
    "        tr_q_loss = 0.0\n",
    "        dqn.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            context = batch[0].to(device)\n",
    "            real_Q = batch[1].to(device)\n",
    "            predicted_Q = dqn(context)\n",
    "            q_optimizer.zero_grad()\n",
    "            q_loss = loss(predicted_Q, real_Q.float())\n",
    "            q_loss.backward()\n",
    "            q_optimizer.step()\n",
    "            tr_q_loss += q_loss.item()\n",
    "            global_step += 1\n",
    "            if max_steps > 0 and global_step > max_steps:\n",
    "                break\n",
    "\n",
    "        if max_steps > 0 and global_step > max_steps:\n",
    "            break\n",
    "        epoch += 1\n",
    "\n",
    "        tr_q_loss /= step + 1\n",
    "\n",
    "        dqn.eval()\n",
    "        q_loss2 = 0.0\n",
    "        sum_q2 = 0.0\n",
    "        predicted = []\n",
    "        actual = []\n",
    "        # Compute performance of the secondary learner after this batch\n",
    "        for step2, batch2 in enumerate(test_dataloader):\n",
    "            features2 = batch2[0].to(device)\n",
    "            real_Q2 = batch2[1].to(device)\n",
    "            predicted_Q2 = dqn(features2)\n",
    "            q_loss2 += test_loss(predicted_Q2, real_Q2).item()\n",
    "            sum_q2 += torch.sum(predicted_Q2).item()\n",
    "            for ei, i in enumerate(predicted_Q2.cpu().detach().numpy()):\n",
    "                predicted.append(i.item())\n",
    "            for ei, i in enumerate(real_Q2.cpu().detach().numpy()):\n",
    "                actual.append(i.item())\n",
    "\n",
    "\n",
    "        print('Avg. predicted test q:', sum_q2 / len(test_dataset))\n",
    "        q_loss2 /= len(test_dataset)\n",
    "        print(\"Train Loss: \", tr_q_loss)\n",
    "        print(\"Test Loss: \", q_loss2)\n",
    "        if q_loss2 < best_test_loss:\n",
    "            # Save if test performance improves\n",
    "            joblib.dump((predicted, actual), \"pred_vs_actual.jbl\")\n",
    "            torch.save(dqn.state_dict(), 'dqn.bin')\n",
    "            print('Saved!')\n",
    "            best_test_loss = q_loss2\n",
    "\n",
    "    return dqn\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    '''Our secondary learner'''\n",
    "    def __init__(self, embeddings):\n",
    "        '''We use a simple convulotional network as our secondary learner'''\n",
    "        super(DQN, self).__init__()\n",
    "        self.embeddings = embeddings\n",
    "        self.conv = nn.Conv1d(embeddings.weight.size(1),256, 3, padding=1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 32),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, context):\n",
    "        '''Forward pass through the secondary learner'''\n",
    "        pooled = torch.max(self.conv(self.embeddings(context).squeeze(1).transpose(1,2)), 2)[0]\n",
    "        Qs = self.fc(pooled)\n",
    "        return Qs.squeeze(1)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, state_path):\n",
    "        '''Load the secondary learner'''\n",
    "        model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        dqn = cls(model.transformer.wte)\n",
    "        state_dict = torch.load(state_path)\n",
    "        dqn.load_state_dict(state_dict)\n",
    "        dqn.embeddings = model.transformer.wte\n",
    "        dqn.embeddings.weight = model.transformer.wte.weight\n",
    "        return dqn\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    set_seed(42)\n",
    "    # Put dataset of pairs here\n",
    "    train_data = joblib.load(\"IGF_values.jbl\") \n",
    "    # Load model and embedding weights\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    embeddings = model.transformer.wte\n",
    "    embeddings.weight = model.transformer.wte.weight\n",
    "    batch_size = 128\n",
    "    # Initialize  using embedding weights\n",
    "    dqn = DQN(embeddings=embeddings)\n",
    "    # Train the secondary learner for 5000 batches ~= 65 epochs\n",
    "    train2(dqn, train_data, 5000, batch_size)\n",
    "    print(\"Secondary learner trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This final cell compares the secondary learner to the baseline method of\n",
    "# standard finetuning. \n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    '''Our secondary learner'''\n",
    "    def __init__(self, embeddings):\n",
    "        '''We use a simple convulotional network as our secondary learner'''\n",
    "        super(DQN, self).__init__()\n",
    "        self.embeddings = embeddings\n",
    "        self.conv = nn.Conv1d(embeddings.weight.size(1),256, 3, padding=1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 32),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, context):\n",
    "        '''Forward pass through the secondary learner'''\n",
    "        pooled = torch.max(self.conv(self.embeddings(context).squeeze(1).transpose(1,2)), 2)[0]\n",
    "        Qs = self.fc(pooled)\n",
    "        return Qs.squeeze(1)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, state_path):\n",
    "        '''Load the secondary learner'''\n",
    "        model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        dqn = cls(model.transformer.wte)\n",
    "        state_dict = torch.load(state_path)\n",
    "        dqn.load_state_dict(state_dict)\n",
    "        dqn.embeddings = model.transformer.wte\n",
    "        dqn.embeddings.weight = model.transformer.wte.weight\n",
    "        return dqn\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def train(train_dataset, test_dataset, val_dataset, max_steps, batch_size, threshold, dqn=None):\n",
    "    \"\"\" Train the model \"\"\"\n",
    "    # Initialize the model\n",
    "    device = torch.device('cuda:0')\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler)\n",
    "\n",
    "    t_total = max_steps\n",
    "    num_train_epochs = max_steps // (len(train_dataset)) + 1\n",
    "    global_step = 0\n",
    "    context = torch.zeros((1, CONTEXT_LEN), dtype=torch.long, device=device)\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2-medium', output_hidden_states=True).to(device)\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    lm_optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5, eps=1e-8)\n",
    "    lm_scheduler = get_linear_schedule_with_warmup(lm_optimizer, 0, t_total)\n",
    "\n",
    "    model.train()\n",
    "    if dqn is not None:\n",
    "        dqn.to(device)\n",
    "        dqn.eval()\n",
    "    contexts = []\n",
    "    examples = 0\n",
    "\n",
    "    observed_qs = []\n",
    "    test_perps = []\n",
    "    repeats = []\n",
    "    val_perps = []\n",
    "    eval_interval = 1\n",
    "    # Compute the performance of the model at the beginning\n",
    "    real_perp = compute_test_perp(test_dataset, model)\n",
    "    test_perps.append(real_perp)\n",
    "    print('Test perplexity, step', global_step, ':', real_perp)\n",
    "    for _ in range(int(num_train_epochs)):\n",
    "        for step, example in enumerate(train_dataloader):\n",
    "            torch.cuda.empty_cache()\n",
    "            # print(example)\n",
    "            if example.size(2) < 1024:\n",
    "                continue\n",
    "            start = random.randint(0, example.size(2) - CONTEXT_LEN - 1)\n",
    "            context[0, :] = example[0, 0, start:start + CONTEXT_LEN]\n",
    "            lm_optimizer.zero_grad()\n",
    "            outputs = model(context, labels=context)\n",
    "\n",
    "            do_backprop = True\n",
    "\n",
    "            if dqn is not None:\n",
    "                predicted_Q = dqn.forward(torch.tensor(context, dtype=torch.long, device=device).unsqueeze(0))[0].item()\n",
    "                observed_qs.append(float(predicted_Q))\n",
    "                # Here we implement the simple non-constant threshold for the predicted IG(X) value\n",
    "                # We will decay the selectivity of our secondary learner filter from \n",
    "                # 1 standard deviation above average to 1 below average after 10 batches.\n",
    "                if global_step == 10:\n",
    "                    threshold = -1\n",
    "                if predicted_Q < threshold:\n",
    "                    #print(\"Skipped context\")\n",
    "                    do_backprop = False\n",
    "            # If we passed the filter, add the context to the batch!\n",
    "            if do_backprop:\n",
    "                contexts.append(np.array(context.cpu()))\n",
    "                lm_loss = outputs[0]\n",
    "                lm_loss.backward()\n",
    "                examples += 1\n",
    "\n",
    "            del outputs\n",
    "            # Once the batch is filled with enough contexts, backprop on the batch.\n",
    "            if examples == batch_size:\n",
    "                torch.cuda.empty_cache()\n",
    "                examples = 0\n",
    "                # Do LM backprop\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n",
    "                lm_optimizer.step()\n",
    "                lm_scheduler.step()  # Update learning rate schedule\n",
    "                global_step += 1\n",
    "                # Compute the performance of the model at this batch\n",
    "                if global_step % eval_interval == 0:\n",
    "                    #real_perp2 = compute_test_perp(val_dataset, model)\n",
    "                    #val_perps.append(real_perp2)\n",
    "                    real_perp = compute_test_perp(test_dataset, model)\n",
    "                    test_perps.append(real_perp)\n",
    "\n",
    "                    print('Test perplexity, step', global_step, ':', real_perp)\n",
    "                    #print('Val perplexity, step', global_step, ':', real_perp2)\n",
    "        # Break out of the loop after 60 batches\n",
    "            if max_steps > 0 and global_step > 60:\n",
    "                break\n",
    "        if max_steps > 0 and global_step > 60:\n",
    "            break\n",
    "    torch.cuda.empty_cache()\n",
    "    # Do some cleaning up so we can reinitialize for the next run of this function\n",
    "    del model\n",
    "    del lm_optimizer\n",
    "    del lm_scheduler\n",
    "    return observed_qs, test_perps, val_perps, contexts, repeats\n",
    "\n",
    "\n",
    "def compute_test_perp(test_data, model):\n",
    "    '''Compute the performnace of the model on held-out data'''\n",
    "    model.eval()\n",
    "    eval_batch_size = 1\n",
    "    device = next(model.parameters()).device\n",
    "    context = torch.zeros((eval_batch_size, CONTEXT_LEN), dtype=torch.long, device=device)\n",
    "    eval_sampler = SequentialSampler(test_data)\n",
    "    eval_dataloader = DataLoader(test_data, sampler=eval_sampler, batch_size=eval_batch_size)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_examples = 0\n",
    "    it = tqdm(eval_dataloader, desc=\"Evaluating\")\n",
    "    for batch in it:\n",
    "        torch.cuda.empty_cache()\n",
    "        # pad\n",
    "        context.zero_()\n",
    "        for i in range(eval_batch_size):\n",
    "            context[i, :] = batch[i]\n",
    "        outputs = model(context, labels=context)\n",
    "        lm_loss = outputs[0]\n",
    "        eval_loss += lm_loss.sum().item()\n",
    "        nb_eval_examples += batch.size(0)\n",
    "        del outputs\n",
    "        del lm_loss\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_examples\n",
    "    perplexity = torch.exp(torch.tensor(eval_loss, device=device))\n",
    "    model.train()\n",
    "    return float(perplexity.item())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Runs 20 runs of Standard Finetuning and 20 runs of IGF using a simple Non-Constant Thresholding Schedule\n",
    "    # Takes about 6 hours\n",
    "    set_seed(42)\n",
    "    train_data = joblib.load(\"tokenized_stories_train_wikitext103.jbl\") # Tokenized train set here\n",
    "    test_data = joblib.load(\"tokenized_stories_test_wikitext103.jbl\") #  Tokenized Test Contexts\n",
    "    val_data = joblib.load(\"objective_set.jbl\") # Additonal Val Set to Test Performance\n",
    "    dqn = []\n",
    "    test_data_sliced = []\n",
    "    # Subsample a test set\n",
    "    for i in range(10):\n",
    "        for test_example in test_data:\n",
    "            if len(test_example[0]) > 1026:\n",
    "                start = random.randint(0, len(test_example[0]) - CONTEXT_LEN - 1)\n",
    "                test_data_sliced.append(test_example[0, start:start + CONTEXT_LEN])\n",
    "    for replica in range(40):\n",
    "        threshold = 1.0\n",
    "        set_seed(replica+1001)\n",
    "        # First 20 runs will be IGF and second 20 will be Standard Finetuning\n",
    "        use_dqn = replica < 20\n",
    "        del dqn\n",
    "        # Load the secondary learner\n",
    "        dqn = DQN.from_pretrained('dqn.bin') # Saved .bin of secondary learner\n",
    "        # Run for this iteration on SF/IGF\n",
    "        qs, eval_perps, val_perps, contexts, repeats = train(train_data, test_data_sliced, val_data, 100000, 16, threshold, dqn=(dqn if\n",
    "                                                                     use_dqn else None))\n",
    "        # Prefixes of saved performance metrics\n",
    "        replica_id = str(replica) + ('igf_wiki' if use_dqn else 'sf_wiki') \n",
    "        # Save run statistics\n",
    "        joblib.dump(np.array(eval_perps), f'eval_perps_{replica_id}.jbl')\n",
    "        qs = []\n",
    "        eval_perps = []\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference of Means p-value at 60: Ttest_indResult(statistic=3.3847121821445145, pvalue=0.0017649745765882902)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fc873706eb8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This final cell plots the comparison between IGF and standard finetuning.\n",
    "def preprocess(num, file, xmax,offset=0):\n",
    "    runs = []\n",
    "    xmin = 0\n",
    "    for i in range(0+offset, num+offset):\n",
    "        runs.append(joblib.load(\"eval_perps_\" + str(i) + file))\n",
    "    for ei, i in enumerate(runs):\n",
    "        min_perp = 200\n",
    "        new_run = []\n",
    "        for ej, j in enumerate(np.array(i[:xmax+1])):\n",
    "            new_run.append(min(min_perp,i[ej]))\n",
    "            min_perp =  min(min_perp,i[ej])\n",
    "        runs[ei] = new_run\n",
    "    runs = np.array(runs)\n",
    "    #print(runs)\n",
    "    runs = np.vstack(runs)\n",
    "    return runs\n",
    "\n",
    "xmin = 0\n",
    "xmax = 61\n",
    "y = list(range(xmin,xmax))\n",
    "runs_naive_wiki = preprocess(20, \"sf_wiki.jbl\", 61, offset=20)\n",
    "runs_igf_wiki = preprocess(20, \"igf_wiki.jbl\", 61, offset=0)\n",
    "y = list(range(xmin,xmax))\n",
    "plt.errorbar(y, np.mean(np.array(runs_naive_wiki)[:,xmin:xmax], axis=0), yerr=np.std(runs_naive_wiki[:,xmin:xmax],axis=0) / np.sqrt(20), label='SF WikiText')\n",
    "plt.errorbar(y, np.mean(np.array(runs_igf_wiki)[:,xmin:xmax], axis=0), yerr=np.std(runs_igf_wiki[:,xmin:xmax],axis=0) / np.sqrt(20), label='IGF WikiText')\n",
    "\n",
    "y = list(range(xmin,xmax))\n",
    "import scipy.stats\n",
    "print(\"Difference of Means p-value at 60:\", scipy.stats.ttest_ind(np.array(runs_naive_wiki)[:,60], np.array(runs_igf_wiki)[:,60], equal_var=False))\n",
    "plt.xlabel(\"Batches\", fontsize=24)\n",
    "plt.ylabel(\"Perplexity\", fontsize=24)\n",
    "plt.title(\"Comparison of Methods: Standard Finetuning vs. IGF\")\n",
    "plt.tick_params(axis=\"x\", labelsize=14)\n",
    "plt.tick_params(axis=\"y\", labelsize=14)\n",
    "plt.legend(fontsize=12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
